{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Workshop 2 - Introduction to Deep Learning\n",
    "The main elements in Pytorch are:\n",
    "* PyTorch Tensors \n",
    "* Mathematical operations\n",
    "* Autograd module\n",
    "\n",
    "Things we did not cover last time:\n",
    "* Optim module\n",
    "* nn modiule\n",
    "\n",
    "Today we will learn how to build your own Deep Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librarys \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "### Pytorch Tensors\n",
    "Tensors are nothing but multidimensional arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import torch\n",
    "\n",
    "# Define a tensor\n",
    "torch.FloatTensor([2])\n",
    "\n",
    "#Try torch.FloatTensor(2), what do you get?\n",
    "\n",
    "#Create a 2x5 matrix with elements from 1 to 10 \n",
    "print(torch.FloatTensor(np.linspace(1,10,10).reshape(2,5)))\n",
    "\n",
    "#Create a random matrix of the size 2x5\n",
    "#(this will be useful later when we would like to initilaize parameters)\n",
    "print(torch.randn(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Operations\n",
    "There are more than 2200 mathematical operations you can use in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set things up \n",
    "x_1 = torch.FloatTensor([10])\n",
    "x_2 = torch.FloatTensor([20])\n",
    "\n",
    "#Try this \n",
    "print(x_2.add(x_1))\n",
    "\n",
    "# now try x_2.add_(x_1) instead, whats the difference?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd module \n",
    "PyTorch uses a technique called automatic differentiation. That is, we have a recorder that records what operations we have performed, and then it replays it backward to compute our gradients. \n",
    "\n",
    "For this to work, we need `Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.FloatTensor([10]), requires_grad = True)\n",
    "y = x**2\n",
    "\n",
    "#Backprop\n",
    "y.backward()\n",
    "\n",
    "#Evaluate grad\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "What is the gradient of f with respect to x at 10 in the following expression? \n",
    "$$y = \\log(x)$$\n",
    "$$z = 2y^2$$\n",
    "$$f = z + 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optim Module\n",
    "``torch.optim`` is a module that implements various optimization algorithms used for building neural networks. Most of the commonly used methods are already supported, so that we don’t have to build them from scratch (unless you want to!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD() #will become useful later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn module\n",
    "PyTorch autograd makes it easy to define computational graphs and take gradients, but raw autograd can be a bit too low-level for defining complex neural networks. This is where the nn module can help.\n",
    "\n",
    "The nn package defines a set of modules, which we can think of as a neural network layer that produces output from input and may have some trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples\n",
    "#Fully connect layer\n",
    "torch.nn.Linear\n",
    "#Rectified Linear unit\n",
    "torch.nn.ReLU\n",
    "#CNN\n",
    "torch.nn.Conv1d\n",
    "#Dropout\n",
    "torch.nn.Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Class\n",
    "Python is an “object-oriented programming language.” Programmers use classes to keep related things together. This is done using the keyword “class,” which is a grouping of object-oriented constructs.\n",
    "\n",
    "### Creating a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a class using the class keyword\n",
    "class Dog:\n",
    "    pass\n",
    "\n",
    "Rocky = Dog()\n",
    "print(Rocky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining attributes and methods\n",
    "A class by itself is of no use unless there is some functionality associated with it. Functionalities are defined by setting attributes, which act as containers for data and functions related to those attributes. Those functions are called methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    sci_name = \"Canis lupus familiaris\"\n",
    "\n",
    "#Instantiate the class Dog and assign it to variable rocky\n",
    "rocky = Dog()\n",
    "print(rocky.sci_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods are functions inside a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods\n",
    "class Dog:\n",
    "    sci_name = \"Canis lupus familiaris\"\n",
    "    \n",
    "    def change_name(self, new_name):\n",
    "        self.sci_name = new_name \n",
    "        \n",
    "rocky = Dog()\n",
    "rocky.change_name(\"I dunno\")\n",
    "print(rocky.sci_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instances attributes and the init method\n",
    "We can also provide the values for the attributes at runtime. This is done by defining the attributes inside the init method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "    \n",
    "    def change_name(self, new_name):\n",
    "        self.name = new_name # now the name is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the object\n",
    "my_dog = Dog(\"rocky\", 10)\n",
    "print(my_dog.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Create a class call Human and input your name and college and define a function that change your name into \"halo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning: 1 layer example\n",
    "\n",
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the librarys\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off with the model itself\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim) # How many input are we using and how many output are we expecting ?\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "input_dim = 1\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create instances of model\n",
    "model = LinearRegressionModel(1,1)\n",
    "\n",
    "#Select Loss Criterion\n",
    "criterion = nn.MSELoss()\n",
    "l_rate = 0.01 #learning rate \n",
    "optimiser = torch.optim.SGD(model.parameters(), lr = l_rate)\n",
    "\n",
    "#Set the number of iteration for optimization\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fake data\n",
    "x_vals = np.random.rand(50)\n",
    "x_train = np.asarray(x_vals,dtype=np.float32).reshape(-1,1)\n",
    "m = 1\n",
    "alpha = np.random.rand(1)\n",
    "beta = np.random.rand(1)\n",
    "y_correct = np.asarray([2*i+m for i in x_vals], dtype=np.float32).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch+=1\n",
    "    \n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_correct))\n",
    "    \n",
    "    #clear grads \n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    #pass a forward\n",
    "    outputs = model.forward(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimiser.step() #update the parameters\n",
    "    print('epoch {},loss {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the Predictions\n",
    "predicted = model.forward(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "\n",
    "plt.plot(x_train, y_correct, 'go', label = 'from data', alpha = .5)\n",
    "plt.plot(x_train, predicted, label = 'prediction', alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now store the predicted values and the true values into the following:\n",
    "one_layer_prediction = model.forward(Variable(torch.from_numpy(x_train), requires_grad = True))\n",
    "one_layer_true_y = Variable(torch.FloatTensor(y_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "There are two major loss functions in Machine Learning. The first one is the **Mean Squared Error Loss (MSE)** and the second one is the **CrossEntropy Loss**\n",
    "\n",
    "## Mean Squared Error Loss\n",
    "$$ MSE_{loss}(y,f) = \\sum_{i=1}^n (y_i - f(x_i))^2 $$\n",
    "\n",
    "## Cross Entropy Loss\n",
    "$${CEloss}(f(x),class) = -\\log\\big(\\frac{\\exp(f(x)[class])}{\\sum_{j}exp(f(x)[j]}\\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the loss object\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall we have an example of 1 layer neural net, we can obtain the performance of our prediction with the following code\n",
    "loss(one_layer_prediction, one_layer_true_y).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will be using the Cross Entropy Loss later in today's challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer \n",
    "Different optimizer will give you different results (but they hopefully should be the same in long run). In this section, we will examine the learning curve of the same network using different optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets generate another dataset \n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)\n",
    "y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))\n",
    "\n",
    "# plot dataset\n",
    "plt.scatter(x.numpy(), y.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time define your own network, name the class (network) \"Net\".\n",
    "class Net(torch.nn.Module):\n",
    "   # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate different Nets\n",
    "net_SGD         = Net(n_feature = 1, n_hidden = 5, n_output=1)\n",
    "net_Momentum    = Net(n_feature = 1, n_hidden = 5, n_output=1)\n",
    "net_RMSprop     = Net(n_feature = 1, n_hidden = 5, n_output=1)\n",
    "net_Adam        = Net(n_feature = 1, n_hidden = 5, n_output=1)\n",
    "nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the hyperparameters\n",
    "LR = 0.05\n",
    "\n",
    "# different optimizers\n",
    "opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)\n",
    "opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\n",
    "opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\n",
    "opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\n",
    "optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, we will use the MSELoss\n",
    "loss_func = torch.nn.MSELoss()\n",
    "losses_his = [[], [], [], []]   # record loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the inputs as variables\n",
    "x = Variable(x)\n",
    "y = Variable(y)\n",
    "\n",
    "# Run the networks and store the errors into the losses_his\n",
    "for t in range(400):\n",
    "    for net, opt, l_his in zip(nets, optimizers, losses_his):\n",
    "        #Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now examine the learning curves\n",
    "labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']\n",
    "for i, l_his in enumerate(losses_his):\n",
    "    plt.plot(l_his, label=labels[i])\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0,0.3])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "In this section we will plot the activation functions out and examine their shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.linspace(-2,2,200))\n",
    "y_relu = F.relu(x)\n",
    "y_sig = F.sigmoid(x)\n",
    "y_tanh = F.tanh(x)\n",
    "y_elu = F.elu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the activation functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally! Some Deep Learning\n",
    "Lets start with an example \n",
    "\n",
    "<img src=\"network_viz.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the packaes first \n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly generate some data\n",
    "x = torch.FloatTensor(np.linspace(1,100,300).reshape(100,3))\n",
    "y = 2*x[:,1] + x[:,2]**2 +x[:,-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch only works on variables, so what should you do here? \n",
    "x,y = Variable(x), Variable(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n_feature = 3, n_hidden = 3, n_output = 1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5) #Smaller learning rate, longer to converge\n",
    "loss_func = torch.nn.MSELoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do prediction\n",
    "net = Net(n_feature = 3, n_hidden = 3, n_output=1)\n",
    "print(net) # show net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.5)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "RMSE = []\n",
    "# run 300 optimization\n",
    "for t in range(300):\n",
    "    prediction = net(x) #feedfoward\n",
    "    RMSE.append(np.sqrt(np.mean((prediction.data.numpy() - y.data.numpy())**2)))\n",
    "\n",
    "    loss = loss_func(prediction, y) #evaluation\n",
    "    \n",
    "    optimizer.zero_grad() #clear gradients for next training \n",
    "    loss.backward() #backpropagation to compute gradients\n",
    "    optimizer.step() #apply the gradients to the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0,299,300)\n",
    "y_axis = RMSE\n",
    "plt.plot(x_axis,y_axis)\n",
    "plt.title('Learning Curve')\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use 10 hidden units instead and compare the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a neural network quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace following class code with an easy sequential network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old way\n",
    "net1 = Net(1, 10, 1)\n",
    "\n",
    "# easy and fast way to build your network\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net1)     # net1 architecture\n",
    "print(net2)     # net2 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge time ! Class predictions for the retail dataset!\n",
    "In this challenge, you will be predicting the class membership of a customer from the retail dataset. Remember to split your dataset into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data using pd.read_csv() and set \"CustomerID\" as the index col\n",
    "retail = pd.read_csv(\"retail_data.csv\", index_col= \"CustomerID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
